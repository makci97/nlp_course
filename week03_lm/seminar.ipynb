{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram language models or how to write scientific papers (4 pts)\n",
    "\n",
    "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
    "\n",
    "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
    "\n",
    "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
    "\n",
    "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-02 17:34:32--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
      "Resolving www.dropbox.com... 162.125.70.1\n",
      "Connecting to www.dropbox.com|162.125.70.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
      "--2019-01-02 17:34:33--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucabccd4f3c23b2fb8afdac17e4b.dl.dropboxusercontent.com/cd/0/get/AYqc9EuFVop62pQIjRWLUE6dyXnmM38FeMCKU2uj1y1trNW3LIeodHFJc5GMQSnxaK4pIJiAFK3ps5m0V-cxk0Myv9TXQLKJGeMV_1PD1u1ZkqE4jrwVO-CW_Z6WZemX97xrdZpmzKqEKgQFCwJGnRbYCCmoa37mzhj-OBfJPCght-DsRvu--N5b_SY7-iP7eWc/file?dl=1 [following]\n",
      "--2019-01-02 17:34:33--  https://ucabccd4f3c23b2fb8afdac17e4b.dl.dropboxusercontent.com/cd/0/get/AYqc9EuFVop62pQIjRWLUE6dyXnmM38FeMCKU2uj1y1trNW3LIeodHFJc5GMQSnxaK4pIJiAFK3ps5m0V-cxk0Myv9TXQLKJGeMV_1PD1u1ZkqE4jrwVO-CW_Z6WZemX97xrdZpmzKqEKgQFCwJGnRbYCCmoa37mzhj-OBfJPCght-DsRvu--N5b_SY7-iP7eWc/file?dl=1\n",
      "Resolving ucabccd4f3c23b2fb8afdac17e4b.dl.dropboxusercontent.com... 162.125.70.6\n",
      "Connecting to ucabccd4f3c23b2fb8afdac17e4b.dl.dropboxusercontent.com|162.125.70.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18933283 (18M) [application/binary]\n",
      "Saving to: ‘arxivData.json.tar.gz’\n",
      "\n",
      "arxivData.json.tar. 100%[===================>]  18.06M  4.79MB/s    in 4.5s    \n",
      "\n",
      "2019-01-02 17:34:39 (4.04 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
      "\n",
      "x arxivData.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>[{'name': 'Mengdi Wang'}, {'name': 'Yichen Che...</td>\n",
       "      <td>12</td>\n",
       "      <td>1511.03760v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>11</td>\n",
       "      <td>Consider convex optimization problems subject ...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Random Multi-Constraint Projection: Stochastic...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24763</th>\n",
       "      <td>[{'name': 'Sung Ju Hwang'}, {'name': 'Leonid S...</td>\n",
       "      <td>18</td>\n",
       "      <td>1411.5879v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>11</td>\n",
       "      <td>We propose a method that learns a discriminati...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>A Unified Semantic Embedding: Relating Taxonom...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39055</th>\n",
       "      <td>[{'name': 'Hamza Bendaoudi'}, {'name': 'Farida...</td>\n",
       "      <td>6</td>\n",
       "      <td>1612.09524v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>12</td>\n",
       "      <td>This paper presents a memory efficient archite...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Memory Efficient Multi-Scale Line Detector Arc...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>[{'name': 'Lin Feng'}, {'name': 'Shuliang Xu'}...</td>\n",
       "      <td>30</td>\n",
       "      <td>1710.10824v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>10</td>\n",
       "      <td>Extreme learning machine (ELM) is a new single...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Rough extreme learning machine: a new classifi...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31522</th>\n",
       "      <td>[{'name': 'Weipeng Xu'}, {'name': 'Avishek Cha...</td>\n",
       "      <td>15</td>\n",
       "      <td>1803.05959v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose the first real-time approach for th...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Mo2Cap2: Real-time Mobile 3D Motion Capture wi...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "11365  [{'name': 'Mengdi Wang'}, {'name': 'Yichen Che...   12  1511.03760v1   \n",
       "24763  [{'name': 'Sung Ju Hwang'}, {'name': 'Leonid S...   18   1411.5879v2   \n",
       "39055  [{'name': 'Hamza Bendaoudi'}, {'name': 'Farida...    6  1612.09524v1   \n",
       "2254   [{'name': 'Lin Feng'}, {'name': 'Shuliang Xu'}...   30  1710.10824v2   \n",
       "31522  [{'name': 'Weipeng Xu'}, {'name': 'Avishek Cha...   15  1803.05959v1   \n",
       "\n",
       "                                                    link  month  \\\n",
       "11365  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
       "24763  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
       "39055  [{'rel': 'alternate', 'href': 'http://arxiv.or...     12   \n",
       "2254   [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
       "31522  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "\n",
       "                                                 summary  \\\n",
       "11365  Consider convex optimization problems subject ...   \n",
       "24763  We propose a method that learns a discriminati...   \n",
       "39055  This paper presents a memory efficient archite...   \n",
       "2254   Extreme learning machine (ELM) is a new single...   \n",
       "31522  We propose the first real-time approach for th...   \n",
       "\n",
       "                                                     tag  \\\n",
       "11365  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "24763  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "39055  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "2254   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
       "31522  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "11365  Random Multi-Constraint Projection: Stochastic...  2015  \n",
       "24763  A Unified Semantic Embedding: Relating Taxonom...  2014  \n",
       "39055  Memory Efficient Multi-Scale Line Detector Arc...  2016  \n",
       "2254   Rough extreme learning machine: a new classifi...  2017  \n",
       "31522  Mo2Cap2: Real-time Mobile 3D Motion Capture wi...  2018  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
    "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "!tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokinizer = WordPunctTokenizer()\n",
    "lines = tokinizer.tokenize_sents([line.lower() for line in lines])\n",
    "lines = list(map(lambda line: ' '.join(line), lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dual recurrent attention units for visual question answering ; we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention . the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question . our single model outperforms the first place winner on the vqa 1 . 0 dataset , performs within margin to the current state - of - the - art ensemble model . we also experiment with replacing attention mechanisms in other state - of - the - art models with our implementation and show increased accuracy . in both cases , our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset .',\n",
       " 'sequential short - text classification with recurrent and convolutional neural networks ; recent approaches based on artificial neural networks ( anns ) have shown promising results for short - text classification . however , many short texts occur in sequences ( e . g ., sentences in a document or utterances in a dialog ), and most existing ann - based systems do not leverage the preceding short texts when classifying a subsequent one . in this work , we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts . our model achieves state - of - the - art results on three different datasets for dialog act prediction .',\n",
       " 'multiresolution recurrent neural networks : an application to dialogue response generation ; we introduce the multiresolution recurrent neural network , which extends the sequence - to - sequence framework to model natural language generation as two parallel discrete stochastic processes : a sequence of high - level coarse tokens , and a sequence of natural language tokens . there are many ways to estimate or learn the high - level coarse tokens , but we argue that a simple extraction procedure is sufficient to capture a wealth of high - level discourse semantics . such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log - likelihood over both sequences . in contrast to the standard log - likelihood objective w . r . t . natural language tokens ( word perplexity ), optimizing the joint log - likelihood biases the model towards modeling high - level abstractions . we apply the proposed model to the task of dialogue response generation in two challenging domains : the ubuntu technical support domain , and twitter conversations . on ubuntu , the model outperforms competing approaches by a substantial margin , achieving state - of - the - art results according to both automatic evaluation metrics and a human evaluation study . on twitter , the model appears to generate more relevant and on - topic responses according to automatic evaluation metrics . finally , our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long - term structure .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(lines, key=len)[0] == \\\n",
    "    'differential contrastive divergence ; this paper has been retracted .'\n",
    "assert sorted(lines, key=len)[2] == \\\n",
    "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Model\n",
    "\n",
    "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "It can do so by following the chain rule:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
    "\n",
    "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
    "\n",
    "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
    "\n",
    "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage to building such a model is counting all word occurences given N-1 previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# special tokens: \n",
    "# - unk represents absent tokens, \n",
    "# - eos is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "\n",
    "    for line in lines:\n",
    "        line = [UNK] * (n - 1) + line.split() + [EOS]\n",
    "        for i in range(n - 1, len(line)):\n",
    "            counts[tuple(line[i - (n - 1): i])][line[i]] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.\n",
    "The simplest way to compute probabilities is in proporiton to counts:\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        self.counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "#         self.probs = defaultdict(Counter)\n",
    "        self.probs = self.counts.copy()\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for prefix in self.counts.keys():\n",
    "            norm_coef = sum(self.probs[prefix][word] for word in self.probs[prefix].keys())\n",
    "            for word in self.probs[prefix].keys():\n",
    "                self.probs[prefix][word] /= norm_coef\n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        return self.probs[self._preproc_prefix(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)\n",
    "    \n",
    "    def _preproc_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a tuple of last tokens (n - 1) tokens in prefix\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return tuple(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 544 ms, total: 21.9 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
    "\n",
    "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 0.12831858407079647),\n",
       " ('networks', 0.12389380530973451),\n",
       " ('machine', 0.11504424778761062),\n",
       " ('-', 0.02654867256637168),\n",
       " ('semantic', 0.017699115044247787),\n",
       " ('architectures', 0.017699115044247787),\n",
       " ('multi', 0.017699115044247787),\n",
       " ('text', 0.01327433628318584),\n",
       " ('variational', 0.01327433628318584),\n",
       " ('program', 0.01327433628318584),\n",
       " ('models', 0.008849557522123894),\n",
       " ('question', 0.008849557522123894),\n",
       " ('architecture', 0.008849557522123894),\n",
       " ('symbolic', 0.008849557522123894),\n",
       " ('style', 0.008849557522123894),\n",
       " ('discourse', 0.008849557522123894),\n",
       " ('system', 0.008849557522123894),\n",
       " ('turing', 0.008849557522123894),\n",
       " ('reranking', 0.008849557522123894),\n",
       " ('net', 0.008849557522123894),\n",
       " ('word', 0.008849557522123894),\n",
       " ('paraphrase', 0.008849557522123894),\n",
       " ('lattice', 0.008849557522123894),\n",
       " ('enquirer', 0.004424778761061947),\n",
       " ('associative', 0.004424778761061947),\n",
       " ('module', 0.004424778761061947),\n",
       " ('photo', 0.004424778761061947),\n",
       " ('responding', 0.004424778761061947),\n",
       " ('domain', 0.004424778761061947),\n",
       " ('speech', 0.004424778761061947),\n",
       " ('tuning', 0.004424778761061947),\n",
       " ('taylor', 0.004424778761061947),\n",
       " ('expectation', 0.004424778761061947),\n",
       " ('component', 0.004424778761061947),\n",
       " ('belief', 0.004424778761061947),\n",
       " ('programmer', 0.004424778761061947),\n",
       " ('tree', 0.004424778761061947),\n",
       " ('probabilistic', 0.004424778761061947),\n",
       " ('optimizer', 0.004424778761061947),\n",
       " ('coarse', 0.004424778761061947),\n",
       " ('combinatorial', 0.004424778761061947),\n",
       " ('block', 0.004424778761061947),\n",
       " ('crf', 0.004424778761061947),\n",
       " ('realisation', 0.004424778761061947),\n",
       " ('aggregation', 0.004424778761061947),\n",
       " ('stain', 0.004424778761061947),\n",
       " ('dynamic', 0.004424778761061947),\n",
       " ('random', 0.004424778761061947),\n",
       " ('decision', 0.004424778761061947),\n",
       " ('episodic', 0.004424778761061947),\n",
       " ('embeddings', 0.004424778761061947),\n",
       " ('sequence', 0.004424778761061947),\n",
       " ('relational', 0.004424778761061947),\n",
       " ('conditional', 0.004424778761061947),\n",
       " ('affine', 0.004424778761061947),\n",
       " ('3d', 0.004424778761061947),\n",
       " ('algebra', 0.004424778761061947),\n",
       " ('rating', 0.004424778761061947),\n",
       " ('self', 0.004424778761061947),\n",
       " ('ranking', 0.004424778761061947),\n",
       " ('voice', 0.004424778761061947),\n",
       " ('audio', 0.004424778761061947),\n",
       " ('slam', 0.004424778761061947),\n",
       " ('task', 0.004424778761061947),\n",
       " ('hypernetwork', 0.004424778761061947),\n",
       " ('nilm', 0.004424778761061947),\n",
       " ('decomposition', 0.004424778761061947),\n",
       " ('distributed', 0.004424778761061947),\n",
       " ('attention', 0.004424778761061947),\n",
       " ('headline', 0.004424778761061947),\n",
       " ('document', 0.004424778761061947),\n",
       " ('emoji', 0.004424778761061947),\n",
       " ('generative', 0.004424778761061947),\n",
       " ('summarization', 0.004424778761061947),\n",
       " ('recovery', 0.004424778761061947),\n",
       " ('morphological', 0.004424778761061947),\n",
       " ('name', 0.004424778761061947),\n",
       " ('sentence', 0.004424778761061947),\n",
       " ('versus', 0.004424778761061947),\n",
       " ('structural', 0.004424778761061947),\n",
       " ('personalized', 0.004424778761061947),\n",
       " ('extractive', 0.004424778761061947),\n",
       " ('end', 0.004424778761061947),\n",
       " ('amr', 0.004424778761061947),\n",
       " ('and', 0.004424778761061947),\n",
       " ('wikipedian', 0.004424778761061947),\n",
       " ('speed', 0.004424778761061947),\n",
       " ('skill', 0.004424778761061947),\n",
       " ('response', 0.004424778761061947),\n",
       " ('cross', 0.004424778761061947),\n",
       " ('fine', 0.004424778761061947),\n",
       " ('granger', 0.004424778761061947),\n",
       " ('perceptual', 0.004424778761061947),\n",
       " ('codes', 0.004424778761061947),\n",
       " ('activation', 0.004424778761061947),\n",
       " ('method', 0.004424778761061947),\n",
       " ('ctrl', 0.004424778761061947),\n",
       " ('face', 0.004424778761061947),\n",
       " ('person', 0.004424778761061947),\n",
       " ('class', 0.004424778761061947),\n",
       " ('color', 0.004424778761061947),\n",
       " ('motifs', 0.004424778761061947),\n",
       " ('signatures', 0.004424778761061947),\n",
       " ('stereoscopic', 0.004424778761061947),\n",
       " ('aesthetic', 0.004424778761061947),\n",
       " ('tensor', 0.004424778761061947),\n",
       " ('daylight', 0.004424778761061947),\n",
       " ('implementation', 0.004424778761061947),\n",
       " ('translation', 0.004424778761061947),\n",
       " ('vector', 0.004424778761061947),\n",
       " ('coordination', 0.004424778761061947),\n",
       " ('attribute', 0.004424778761061947),\n",
       " ('predictive', 0.004424778761061947)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k, v) for k, v in lm.get_possible_next_tokens(\"neural\").items()], key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gap', 0.7435897435897436),\n",
       " ('synthetic', 0.02564102564102564),\n",
       " ('language', 0.02564102564102564),\n",
       " ('subsymbolic', 0.02564102564102564),\n",
       " ('relatively', 0.02564102564102564),\n",
       " ('difference', 0.02564102564102564),\n",
       " ('rbm', 0.02564102564102564),\n",
       " ('disambiguated', 0.02564102564102564),\n",
       " ('information', 0.02564102564102564),\n",
       " ('insights', 0.02564102564102564),\n",
       " ('topological', 0.02564102564102564)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k, v) for k, v in lm.get_possible_next_tokens(\"bridging the\").items()], key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ temperature\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    token_probs = lm.get_possible_next_tokens(prefix)\n",
    "    if temperature == 0:\n",
    "        probs = np.zeros(len(token_probs))\n",
    "        probs[np.argmax(list(token_probs.values()))] = 1\n",
    "    else:\n",
    "        degree = 1 / (temperature + 1e-10)\n",
    "        probs = np.array([v**degree for v in token_probs.values()])\n",
    "        probs = probs / sum(probs)\n",
    "    return np.random.choice(list(token_probs.keys()), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have fun with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence is essentially an algorithm is an improved entropy number bound by doerr and doerr ( gecco 2015 ). the new digital synaptic neural substrate ' ( ogs ) regularization is important for improving end of the objective of each domain , we also release object bounding box coordinates and the voice structure of the image processing ; we present an algorithm ( mnih et al .' s to 1970 ' s new assertion , or in a multilingual corpus used in image retrieval . in this paper , we complement our generalization is concerned with recovery from uncalibrated cameras\n"
     ]
    }
   ],
   "source": [
    "prefix = 'artificial' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the two models are the best performing models . we show that the proposed method is based on the imagenet dataset . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between these two modalities generally leads to a wide range of problems that are used for the purpose of this paper , we propose a new model , we propose a new objective function . the proposed method is that the proposed method is also presented . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating language models: perplexity\n",
    "\n",
    "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
    "\n",
    "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' a'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    log_perplexity = 0 \n",
    "    for line in lines:\n",
    "        prefix = ''\n",
    "        for token in line.split() + [EOS]:\n",
    "            log_perplexity += max(np.log(lm.get_next_token_prob(prefix, token)), min_logprob)\n",
    "            prefix += ' ' + token\n",
    "            n += 1\n",
    "    return np.exp(-log_perplexity / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxim/code/git/kaggle/kaggle_env/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n",
    "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxim/code/git/kaggle/kaggle_env/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23136\n",
      "N = 2, Perplexity = 85653987.28774\n",
      "N = 3, Perplexity = 61999196259043346743296.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoops, it just blew up :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Smoothing\n",
    "\n",
    "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
    "\n",
    "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
    "\n",
    "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distrivution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
    "\n",
    "Here's an example code we've implemented for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 977.67559\n",
      "N = 2, Perplexity = 470.48021\n",
      "N = 3, Perplexity = 3679.44765\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for n in tqdm((1, 2, 3)):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:31<01:02, 31.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 977.93963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [01:04<00:32, 32.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 290.88339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3/3 [01:48<00:00, 35.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 1573.42741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for n in tqdm((1, 2, 3)):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.01)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/maxim/code/git/kaggle/kaggle_env/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      " 33%|███▎      | 1/3 [00:31<01:03, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [01:04<00:32, 32.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 282.27785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3/3 [01:48<00:00, 35.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 1073.32808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for n in tqdm((1, 2, 3)):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.001)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LaplaceLanguageModel(lines, n=3, delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gap', 0.28499130315150206),\n",
       " ('insights', 0.009836774402767267),\n",
       " ('disambiguated', 0.009836774402767267),\n",
       " ('rbm', 0.009836774402767267),\n",
       " ('topological', 0.009836774402767267),\n",
       " ('difference', 0.009836774402767267),\n",
       " ('relatively', 0.009836774402767267),\n",
       " ('information', 0.009836774402767267),\n",
       " ('synthetic', 0.009836774402767267),\n",
       " ('subsymbolic', 0.009836774402767267),\n",
       " ('language', 0.009836774402767267),\n",
       " ('dmds', 9.826947455311959e-06),\n",
       " ('roundworms', 9.826947455311959e-06),\n",
       " ('recalibrates', 9.826947455311959e-06),\n",
       " ('typhoons', 9.826947455311959e-06),\n",
       " ('velopment', 9.826947455311959e-06),\n",
       " ('@', 9.826947455311959e-06),\n",
       " ('irl2', 9.826947455311959e-06),\n",
       " ('58', 9.826947455311959e-06),\n",
       " ('formalise', 9.826947455311959e-06),\n",
       " ('supervoxel', 9.826947455311959e-06),\n",
       " ('picture', 9.826947455311959e-06),\n",
       " ('multilevel', 9.826947455311959e-06),\n",
       " ('grobner', 9.826947455311959e-06),\n",
       " ('atomnet', 9.826947455311959e-06),\n",
       " ('horn', 9.826947455311959e-06),\n",
       " ('utilising', 9.826947455311959e-06),\n",
       " ('monty', 9.826947455311959e-06),\n",
       " ('nagao', 9.826947455311959e-06),\n",
       " ('isnr', 9.826947455311959e-06),\n",
       " ('blendshapes', 9.826947455311959e-06),\n",
       " ('remnants', 9.826947455311959e-06),\n",
       " ('avlsi', 9.826947455311959e-06),\n",
       " ('dixon', 9.826947455311959e-06),\n",
       " ('expérience', 9.826947455311959e-06),\n",
       " ('swir', 9.826947455311959e-06),\n",
       " ('wsvrd', 9.826947455311959e-06),\n",
       " ('hallucinating', 9.826947455311959e-06),\n",
       " ('nspr', 9.826947455311959e-06),\n",
       " ('iem', 9.826947455311959e-06),\n",
       " ('diacritical', 9.826947455311959e-06),\n",
       " ('dmoea', 9.826947455311959e-06),\n",
       " ('ls', 9.826947455311959e-06),\n",
       " ('ng', 9.826947455311959e-06),\n",
       " ('recaptured', 9.826947455311959e-06),\n",
       " ('routines', 9.826947455311959e-06),\n",
       " ('rfit', 9.826947455311959e-06),\n",
       " ('literature', 9.826947455311959e-06),\n",
       " ('projector', 9.826947455311959e-06),\n",
       " ('2007b', 9.826947455311959e-06),\n",
       " ('endurantism', 9.826947455311959e-06),\n",
       " ('sophisticatedly', 9.826947455311959e-06),\n",
       " ('toot', 9.826947455311959e-06),\n",
       " ('chaplin', 9.826947455311959e-06),\n",
       " ('braziliensis', 9.826947455311959e-06),\n",
       " ('mavot', 9.826947455311959e-06),\n",
       " ('blanks', 9.826947455311959e-06),\n",
       " ('dependability', 9.826947455311959e-06),\n",
       " ('persistence', 9.826947455311959e-06),\n",
       " ('underextension', 9.826947455311959e-06),\n",
       " ('monologue', 9.826947455311959e-06),\n",
       " ('approvals', 9.826947455311959e-06),\n",
       " ('ist', 9.826947455311959e-06),\n",
       " ('steep', 9.826947455311959e-06),\n",
       " ('sigma_i', 9.826947455311959e-06),\n",
       " ('communications', 9.826947455311959e-06),\n",
       " ('submits', 9.826947455311959e-06),\n",
       " ('inria', 9.826947455311959e-06),\n",
       " ('larger', 9.826947455311959e-06),\n",
       " ('conduciveness', 9.826947455311959e-06),\n",
       " ('grt', 9.826947455311959e-06),\n",
       " ('viewers', 9.826947455311959e-06),\n",
       " ('supercomputers', 9.826947455311959e-06),\n",
       " ('keputusan', 9.826947455311959e-06),\n",
       " ('lightheart', 9.826947455311959e-06),\n",
       " ('bessel', 9.826947455311959e-06),\n",
       " ('delhi', 9.826947455311959e-06),\n",
       " ('municipalities', 9.826947455311959e-06),\n",
       " ('teristics', 9.826947455311959e-06),\n",
       " ('guinea', 9.826947455311959e-06),\n",
       " ('intphys', 9.826947455311959e-06),\n",
       " ('emoji2vec', 9.826947455311959e-06),\n",
       " ('maxpools', 9.826947455311959e-06),\n",
       " ('fabricate', 9.826947455311959e-06),\n",
       " ('arries', 9.826947455311959e-06),\n",
       " ('cognitives', 9.826947455311959e-06),\n",
       " ('bats', 9.826947455311959e-06),\n",
       " ('216k', 9.826947455311959e-06),\n",
       " ('human3', 9.826947455311959e-06),\n",
       " ('corl', 9.826947455311959e-06),\n",
       " ('effectuer', 9.826947455311959e-06),\n",
       " ('combinable', 9.826947455311959e-06),\n",
       " ('impact', 9.826947455311959e-06),\n",
       " ('eol', 9.826947455311959e-06),\n",
       " ('impacting', 9.826947455311959e-06),\n",
       " ('121', 9.826947455311959e-06),\n",
       " ('bentley', 9.826947455311959e-06),\n",
       " ('poisoning', 9.826947455311959e-06),\n",
       " ('jointed', 9.826947455311959e-06),\n",
       " ('margins', 9.826947455311959e-06),\n",
       " ('*},\\\\', 9.826947455311959e-06),\n",
       " ('pushdown', 9.826947455311959e-06),\n",
       " ('ale', 9.826947455311959e-06),\n",
       " ('webcl', 9.826947455311959e-06),\n",
       " ('reapply', 9.826947455311959e-06),\n",
       " ('liquidsvm', 9.826947455311959e-06),\n",
       " ('hxu', 9.826947455311959e-06),\n",
       " ('endeavours', 9.826947455311959e-06),\n",
       " ('rashness', 9.826947455311959e-06),\n",
       " ('icdar', 9.826947455311959e-06),\n",
       " ('transports', 9.826947455311959e-06),\n",
       " ('\\\\}}', 9.826947455311959e-06),\n",
       " ('risky', 9.826947455311959e-06),\n",
       " ('gale', 9.826947455311959e-06),\n",
       " ('boolean', 9.826947455311959e-06),\n",
       " ('stockbrokers', 9.826947455311959e-06),\n",
       " ('166898', 9.826947455311959e-06),\n",
       " ('anlaytics', 9.826947455311959e-06),\n",
       " ('pentomino', 9.826947455311959e-06),\n",
       " ('tfusion', 9.826947455311959e-06),\n",
       " ('youtube2text', 9.826947455311959e-06),\n",
       " ('parsers', 9.826947455311959e-06),\n",
       " ('classfication', 9.826947455311959e-06),\n",
       " ('augchemception', 9.826947455311959e-06),\n",
       " ('mtmc', 9.826947455311959e-06),\n",
       " ('fibre', 9.826947455311959e-06),\n",
       " ('superiror', 9.826947455311959e-06),\n",
       " ('inhibiting', 9.826947455311959e-06),\n",
       " ('cyfrowych', 9.826947455311959e-06),\n",
       " ('lee', 9.826947455311959e-06),\n",
       " ('penetrating', 9.826947455311959e-06),\n",
       " ('readiness', 9.826947455311959e-06),\n",
       " ('7972', 9.826947455311959e-06),\n",
       " ('rpe', 9.826947455311959e-06),\n",
       " ('lasted', 9.826947455311959e-06),\n",
       " ('snmf', 9.826947455311959e-06),\n",
       " ('872', 9.826947455311959e-06),\n",
       " ('owl', 9.826947455311959e-06),\n",
       " ('cubed', 9.826947455311959e-06),\n",
       " ('globalnet', 9.826947455311959e-06),\n",
       " ('exchangeability', 9.826947455311959e-06),\n",
       " ('scalably', 9.826947455311959e-06),\n",
       " ('tidsets', 9.826947455311959e-06),\n",
       " ('publishable', 9.826947455311959e-06),\n",
       " ('cctv', 9.826947455311959e-06),\n",
       " ('gprm', 9.826947455311959e-06),\n",
       " ('diaz', 9.826947455311959e-06),\n",
       " ('bdnns', 9.826947455311959e-06),\n",
       " ('miscommunication', 9.826947455311959e-06),\n",
       " ('trpo', 9.826947455311959e-06),\n",
       " ('lh3', 9.826947455311959e-06),\n",
       " ('netime', 9.826947455311959e-06),\n",
       " ('--\"', 9.826947455311959e-06),\n",
       " ('jitai', 9.826947455311959e-06),\n",
       " ('citeseer', 9.826947455311959e-06),\n",
       " ('mufurus', 9.826947455311959e-06),\n",
       " ('conseqopt', 9.826947455311959e-06),\n",
       " ('disentailment', 9.826947455311959e-06),\n",
       " ('leaks', 9.826947455311959e-06),\n",
       " ('conceptor', 9.826947455311959e-06),\n",
       " ('qsat', 9.826947455311959e-06),\n",
       " ('htdn', 9.826947455311959e-06),\n",
       " ('deephash', 9.826947455311959e-06),\n",
       " ('geodetic', 9.826947455311959e-06),\n",
       " ('caffe_lenet', 9.826947455311959e-06),\n",
       " ('mend', 9.826947455311959e-06),\n",
       " ('nyt', 9.826947455311959e-06),\n",
       " ('outlooks', 9.826947455311959e-06),\n",
       " ('i7', 9.826947455311959e-06),\n",
       " ('u_xxx', 9.826947455311959e-06),\n",
       " ('troubleshotting', 9.826947455311959e-06),\n",
       " ('picanets', 9.826947455311959e-06),\n",
       " ('coworkers', 9.826947455311959e-06),\n",
       " ('openmt12', 9.826947455311959e-06),\n",
       " ('corsica', 9.826947455311959e-06),\n",
       " ('physiologic', 9.826947455311959e-06),\n",
       " ('fbdds', 9.826947455311959e-06),\n",
       " ('wick', 9.826947455311959e-06),\n",
       " (')$),', 9.826947455311959e-06),\n",
       " ('interchange', 9.826947455311959e-06),\n",
       " ('demonstrates', 9.826947455311959e-06),\n",
       " ('collision', 9.826947455311959e-06),\n",
       " ('pencil', 9.826947455311959e-06),\n",
       " ('quartets', 9.826947455311959e-06),\n",
       " ('kovvalsky', 9.826947455311959e-06),\n",
       " ('greg', 9.826947455311959e-06),\n",
       " ('completability', 9.826947455311959e-06),\n",
       " ('uninterrupted', 9.826947455311959e-06),\n",
       " ('254', 9.826947455311959e-06),\n",
       " ('contradictory', 9.826947455311959e-06),\n",
       " ('mcsm', 9.826947455311959e-06),\n",
       " ('paucity', 9.826947455311959e-06),\n",
       " ('pseudodimension', 9.826947455311959e-06),\n",
       " ('freamework', 9.826947455311959e-06),\n",
       " ('interreflection', 9.826947455311959e-06),\n",
       " ('difined', 9.826947455311959e-06),\n",
       " ('intangibility', 9.826947455311959e-06),\n",
       " ('adjec', 9.826947455311959e-06),\n",
       " ('lism', 9.826947455311959e-06),\n",
       " ('herschel', 9.826947455311959e-06),\n",
       " ('adpm', 9.826947455311959e-06),\n",
       " ('spath1979', 9.826947455311959e-06),\n",
       " ('mammals', 9.826947455311959e-06),\n",
       " ('nonconvexity', 9.826947455311959e-06),\n",
       " ('polyadic', 9.826947455311959e-06),\n",
       " ('defocused', 9.826947455311959e-06),\n",
       " ('bruce', 9.826947455311959e-06),\n",
       " ('port', 9.826947455311959e-06),\n",
       " ('rectifiers', 9.826947455311959e-06),\n",
       " ('demanding', 9.826947455311959e-06),\n",
       " ('murmurhash3', 9.826947455311959e-06),\n",
       " ('claimant', 9.826947455311959e-06),\n",
       " ('russe', 9.826947455311959e-06),\n",
       " ('vious', 9.826947455311959e-06),\n",
       " ('defraud', 9.826947455311959e-06),\n",
       " ('shivashan', 9.826947455311959e-06),\n",
       " ('sceneflowfields', 9.826947455311959e-06),\n",
       " ('quoted', 9.826947455311959e-06),\n",
       " ('randsvm', 9.826947455311959e-06),\n",
       " ('indistinct', 9.826947455311959e-06),\n",
       " ('multiply', 9.826947455311959e-06),\n",
       " ('flemish', 9.826947455311959e-06),\n",
       " ('reduc', 9.826947455311959e-06),\n",
       " ('destructed', 9.826947455311959e-06),\n",
       " ('umd', 9.826947455311959e-06),\n",
       " ('gospel', 9.826947455311959e-06),\n",
       " ('distractor', 9.826947455311959e-06),\n",
       " ('manufacturers', 9.826947455311959e-06),\n",
       " ('bikes', 9.826947455311959e-06),\n",
       " ('rovotics', 9.826947455311959e-06),\n",
       " ('demonstrations', 9.826947455311959e-06),\n",
       " ('isophotes', 9.826947455311959e-06),\n",
       " ('repeated', 9.826947455311959e-06),\n",
       " ('negotiation', 9.826947455311959e-06),\n",
       " ('adacluster', 9.826947455311959e-06),\n",
       " ('enduring', 9.826947455311959e-06),\n",
       " ('popes', 9.826947455311959e-06),\n",
       " ('checkout', 9.826947455311959e-06),\n",
       " ('korf', 9.826947455311959e-06),\n",
       " ('isomorphically', 9.826947455311959e-06),\n",
       " ('arbres', 9.826947455311959e-06),\n",
       " ('k_1', 9.826947455311959e-06),\n",
       " ('struggles', 9.826947455311959e-06),\n",
       " ('softness', 9.826947455311959e-06),\n",
       " ('hypernetwork', 9.826947455311959e-06),\n",
       " ('numeraire', 9.826947455311959e-06),\n",
       " ('mispositioned', 9.826947455311959e-06),\n",
       " ('2sat', 9.826947455311959e-06),\n",
       " ('beispiel', 9.826947455311959e-06),\n",
       " ('wale', 9.826947455311959e-06),\n",
       " ('[{\\\\', 9.826947455311959e-06),\n",
       " ('strategyproofness', 9.826947455311959e-06),\n",
       " ('1485', 9.826947455311959e-06),\n",
       " ('ddff', 9.826947455311959e-06),\n",
       " ('icic', 9.826947455311959e-06),\n",
       " ('player', 9.826947455311959e-06),\n",
       " ('fepll', 9.826947455311959e-06),\n",
       " ('href', 9.826947455311959e-06),\n",
       " ('eto', 9.826947455311959e-06),\n",
       " ('havre', 9.826947455311959e-06),\n",
       " ('utilização', 9.826947455311959e-06),\n",
       " ('dyskinesia', 9.826947455311959e-06),\n",
       " ('dicotyledonous', 9.826947455311959e-06),\n",
       " ('iksc', 9.826947455311959e-06),\n",
       " ('esra', 9.826947455311959e-06),\n",
       " ('maurice', 9.826947455311959e-06),\n",
       " ('odm', 9.826947455311959e-06),\n",
       " ('takeuchi', 9.826947455311959e-06),\n",
       " ('pew', 9.826947455311959e-06),\n",
       " ('lz', 9.826947455311959e-06),\n",
       " ('mnar', 9.826947455311959e-06),\n",
       " ('subdivision', 9.826947455311959e-06),\n",
       " ('amelioration', 9.826947455311959e-06),\n",
       " ('kann', 9.826947455311959e-06),\n",
       " ('sk_p', 9.826947455311959e-06),\n",
       " ('bbse', 9.826947455311959e-06),\n",
       " ('qcqp', 9.826947455311959e-06),\n",
       " ('^*|', 9.826947455311959e-06),\n",
       " ('promoting', 9.826947455311959e-06),\n",
       " ('favorita', 9.826947455311959e-06),\n",
       " ('inflicts', 9.826947455311959e-06),\n",
       " ('presets', 9.826947455311959e-06),\n",
       " ('stcrf', 9.826947455311959e-06),\n",
       " ('temper', 9.826947455311959e-06),\n",
       " ('conspiratorial', 9.826947455311959e-06),\n",
       " ('platt', 9.826947455311959e-06),\n",
       " ('sufficently', 9.826947455311959e-06),\n",
       " ('determiner', 9.826947455311959e-06),\n",
       " ('interceptor', 9.826947455311959e-06),\n",
       " ('fried', 9.826947455311959e-06),\n",
       " ('amortized', 9.826947455311959e-06),\n",
       " ('irrelevant', 9.826947455311959e-06),\n",
       " ('8645', 9.826947455311959e-06),\n",
       " ('expectiles', 9.826947455311959e-06),\n",
       " ('compounded', 9.826947455311959e-06),\n",
       " ('warshall', 9.826947455311959e-06),\n",
       " ('tremendously', 9.826947455311959e-06),\n",
       " ('bonnet', 9.826947455311959e-06),\n",
       " ('delicious', 9.826947455311959e-06),\n",
       " ('scenery', 9.826947455311959e-06),\n",
       " ('autore', 9.826947455311959e-06),\n",
       " ('dazu', 9.826947455311959e-06),\n",
       " ('ltr', 9.826947455311959e-06),\n",
       " ('fpds', 9.826947455311959e-06),\n",
       " ('opacities', 9.826947455311959e-06),\n",
       " ('maria', 9.826947455311959e-06),\n",
       " ('fshbmap', 9.826947455311959e-06),\n",
       " ('trd', 9.826947455311959e-06),\n",
       " ('retor', 9.826947455311959e-06),\n",
       " ('recreate', 9.826947455311959e-06),\n",
       " ('combinatorially', 9.826947455311959e-06),\n",
       " ('noncoding', 9.826947455311959e-06),\n",
       " ('listnet', 9.826947455311959e-06),\n",
       " ('joshi', 9.826947455311959e-06),\n",
       " ('krai', 9.826947455311959e-06),\n",
       " ('krk', 9.826947455311959e-06),\n",
       " ('income', 9.826947455311959e-06),\n",
       " ('743', 9.826947455311959e-06),\n",
       " ('brickerbot', 9.826947455311959e-06),\n",
       " ('classic', 9.826947455311959e-06),\n",
       " ('credit', 9.826947455311959e-06),\n",
       " ('send', 9.826947455311959e-06),\n",
       " ('beach', 9.826947455311959e-06),\n",
       " ('mdds', 9.826947455311959e-06),\n",
       " ('aci', 9.826947455311959e-06),\n",
       " ('tempo', 9.826947455311959e-06),\n",
       " ('ternal', 9.826947455311959e-06),\n",
       " ('sql3', 9.826947455311959e-06),\n",
       " ('redwood', 9.826947455311959e-06),\n",
       " ('andez', 9.826947455311959e-06),\n",
       " ('introduces', 9.826947455311959e-06),\n",
       " ('nickisch', 9.826947455311959e-06),\n",
       " ('drowned', 9.826947455311959e-06),\n",
       " ('affirm', 9.826947455311959e-06),\n",
       " ('oar', 9.826947455311959e-06),\n",
       " ('inex', 9.826947455311959e-06),\n",
       " ('authorships', 9.826947455311959e-06),\n",
       " ('ghorbel', 9.826947455311959e-06),\n",
       " ('taxonomical', 9.826947455311959e-06),\n",
       " ('dapip', 9.826947455311959e-06),\n",
       " ('backreferencing', 9.826947455311959e-06),\n",
       " ('vmf', 9.826947455311959e-06),\n",
       " ('emotxt', 9.826947455311959e-06),\n",
       " ('producers', 9.826947455311959e-06),\n",
       " ('quicknat', 9.826947455311959e-06),\n",
       " ('parameterizable', 9.826947455311959e-06),\n",
       " ('genericness', 9.826947455311959e-06),\n",
       " ('staleness', 9.826947455311959e-06),\n",
       " ('neuralese', 9.826947455311959e-06),\n",
       " ('mediates', 9.826947455311959e-06),\n",
       " ('regularises', 9.826947455311959e-06),\n",
       " ('install', 9.826947455311959e-06),\n",
       " ('supplant', 9.826947455311959e-06),\n",
       " ('underly', 9.826947455311959e-06),\n",
       " ('stereotype', 9.826947455311959e-06),\n",
       " ('dawned', 9.826947455311959e-06),\n",
       " ('neuromodulators', 9.826947455311959e-06),\n",
       " ('analysed', 9.826947455311959e-06),\n",
       " ('swapping', 9.826947455311959e-06),\n",
       " ('middle', 9.826947455311959e-06),\n",
       " ('colorized', 9.826947455311959e-06),\n",
       " ('\\\\,', 9.826947455311959e-06),\n",
       " ('miou', 9.826947455311959e-06),\n",
       " ('secondly', 9.826947455311959e-06),\n",
       " ('egr', 9.826947455311959e-06),\n",
       " ('discrimination', 9.826947455311959e-06),\n",
       " ('pigmentation', 9.826947455311959e-06),\n",
       " ('heriot', 9.826947455311959e-06),\n",
       " ('ljubljana', 9.826947455311959e-06),\n",
       " ('hollunder', 9.826947455311959e-06),\n",
       " ('gr2rss', 9.826947455311959e-06),\n",
       " ('comparedusing', 9.826947455311959e-06),\n",
       " ('citeulike', 9.826947455311959e-06),\n",
       " ('obligatory', 9.826947455311959e-06),\n",
       " ('questionnaire', 9.826947455311959e-06),\n",
       " ('trackers', 9.826947455311959e-06),\n",
       " ('hasn', 9.826947455311959e-06),\n",
       " ('preferred', 9.826947455311959e-06),\n",
       " ('suggests', 9.826947455311959e-06),\n",
       " ('oe', 9.826947455311959e-06),\n",
       " ('sition', 9.826947455311959e-06),\n",
       " ('traffickers', 9.826947455311959e-06),\n",
       " ('compressão', 9.826947455311959e-06),\n",
       " ('onlieb', 9.826947455311959e-06),\n",
       " ('aquire', 9.826947455311959e-06),\n",
       " ('illustrative', 9.826947455311959e-06),\n",
       " ('leaned', 9.826947455311959e-06),\n",
       " ('ubc', 9.826947455311959e-06),\n",
       " ('sensegen', 9.826947455311959e-06),\n",
       " ('maxf', 9.826947455311959e-06),\n",
       " ('iog', 9.826947455311959e-06),\n",
       " ('enlightening', 9.826947455311959e-06),\n",
       " ('_w', 9.826947455311959e-06),\n",
       " ('crosstalks', 9.826947455311959e-06),\n",
       " ('hankel', 9.826947455311959e-06),\n",
       " ('inspirational', 9.826947455311959e-06),\n",
       " ('piare', 9.826947455311959e-06),\n",
       " ('infomax', 9.826947455311959e-06),\n",
       " ('eterized', 9.826947455311959e-06),\n",
       " ('bahn', 9.826947455311959e-06),\n",
       " ('ha98', 9.826947455311959e-06),\n",
       " ('modulations', 9.826947455311959e-06),\n",
       " ('defied', 9.826947455311959e-06),\n",
       " ('hcs', 9.826947455311959e-06),\n",
       " ('properties', 9.826947455311959e-06),\n",
       " ('crosssectional', 9.826947455311959e-06),\n",
       " ('mathematical', 9.826947455311959e-06),\n",
       " ('diffuse', 9.826947455311959e-06),\n",
       " ('variograms', 9.826947455311959e-06),\n",
       " ('dl4mt1', 9.826947455311959e-06),\n",
       " ('madina', 9.826947455311959e-06),\n",
       " ('microbatching', 9.826947455311959e-06),\n",
       " ('sous', 9.826947455311959e-06),\n",
       " ('reptile', 9.826947455311959e-06),\n",
       " ('atelectasis', 9.826947455311959e-06),\n",
       " ('nonhierarchical', 9.826947455311959e-06),\n",
       " ('fetching', 9.826947455311959e-06),\n",
       " ('lidioms', 9.826947455311959e-06),\n",
       " ('sentiwords', 9.826947455311959e-06),\n",
       " ('aside', 9.826947455311959e-06),\n",
       " ('oupms', 9.826947455311959e-06),\n",
       " ('rohlfing', 9.826947455311959e-06),\n",
       " ('binhexed', 9.826947455311959e-06),\n",
       " ('following', 9.826947455311959e-06),\n",
       " ('greedier', 9.826947455311959e-06),\n",
       " ('fluoroscope', 9.826947455311959e-06),\n",
       " ('collocational', 9.826947455311959e-06),\n",
       " ('tied', 9.826947455311959e-06),\n",
       " ('reimagined', 9.826947455311959e-06),\n",
       " ('mapless', 9.826947455311959e-06),\n",
       " ('rigs', 9.826947455311959e-06),\n",
       " ('entangle', 9.826947455311959e-06),\n",
       " ('supper', 9.826947455311959e-06),\n",
       " ('10b', 9.826947455311959e-06),\n",
       " ('dicionaries', 9.826947455311959e-06),\n",
       " ('gridworlds', 9.826947455311959e-06),\n",
       " ('lshtc', 9.826947455311959e-06),\n",
       " ('labels', 9.826947455311959e-06),\n",
       " ('born', 9.826947455311959e-06),\n",
       " ('onelook', 9.826947455311959e-06),\n",
       " ('whi', 9.826947455311959e-06),\n",
       " ('ilsmt', 9.826947455311959e-06),\n",
       " ('isnotes', 9.826947455311959e-06),\n",
       " ('godec', 9.826947455311959e-06),\n",
       " ('$<$', 9.826947455311959e-06),\n",
       " ('acknowledgments', 9.826947455311959e-06),\n",
       " ('galois', 9.826947455311959e-06),\n",
       " ('spanning', 9.826947455311959e-06),\n",
       " ('supervision', 9.826947455311959e-06),\n",
       " ('dissolve', 9.826947455311959e-06),\n",
       " ('858', 9.826947455311959e-06),\n",
       " ('clicktionary', 9.826947455311959e-06),\n",
       " ('fersml', 9.826947455311959e-06),\n",
       " ('xploit', 9.826947455311959e-06),\n",
       " ('mms', 9.826947455311959e-06),\n",
       " ('scantily', 9.826947455311959e-06),\n",
       " ('atleast', 9.826947455311959e-06),\n",
       " ('satirical', 9.826947455311959e-06),\n",
       " ('neco', 9.826947455311959e-06),\n",
       " ('intramodal', 9.826947455311959e-06),\n",
       " ('baek', 9.826947455311959e-06),\n",
       " ('rskc', 9.826947455311959e-06),\n",
       " ('?!', 9.826947455311959e-06),\n",
       " ('locnet', 9.826947455311959e-06),\n",
       " ('microstructure', 9.826947455311959e-06),\n",
       " ('inferential', 9.826947455311959e-06),\n",
       " ('ndi', 9.826947455311959e-06),\n",
       " ('postulated', 9.826947455311959e-06),\n",
       " ('kbmt', 9.826947455311959e-06),\n",
       " ('upsampled', 9.826947455311959e-06),\n",
       " ('regularizations', 9.826947455311959e-06),\n",
       " ('$\\\\%$,', 9.826947455311959e-06),\n",
       " ('monaural', 9.826947455311959e-06),\n",
       " ('wmsn', 9.826947455311959e-06),\n",
       " ('amplifier', 9.826947455311959e-06),\n",
       " ('ca', 9.826947455311959e-06),\n",
       " ('gui', 9.826947455311959e-06),\n",
       " ('plano', 9.826947455311959e-06),\n",
       " ('smsr', 9.826947455311959e-06),\n",
       " ('attackson', 9.826947455311959e-06),\n",
       " ('beamforming', 9.826947455311959e-06),\n",
       " ('stereovision', 9.826947455311959e-06),\n",
       " ('existence', 9.826947455311959e-06),\n",
       " ('{{', 9.826947455311959e-06),\n",
       " ('omenos', 9.826947455311959e-06),\n",
       " ('rumour', 9.826947455311959e-06),\n",
       " ('hearts', 9.826947455311959e-06),\n",
       " ('corporal', 9.826947455311959e-06),\n",
       " ('driverless', 9.826947455311959e-06),\n",
       " ('propagable', 9.826947455311959e-06),\n",
       " ('tscn', 9.826947455311959e-06),\n",
       " ('313', 9.826947455311959e-06),\n",
       " ('influece', 9.826947455311959e-06),\n",
       " ('uninhibitedly', 9.826947455311959e-06),\n",
       " ('through', 9.826947455311959e-06),\n",
       " ('portraitures', 9.826947455311959e-06),\n",
       " ('imagerie', 9.826947455311959e-06),\n",
       " ('naturales', 9.826947455311959e-06),\n",
       " ('shadings', 9.826947455311959e-06),\n",
       " ('sustainably', 9.826947455311959e-06),\n",
       " ('hlda', 9.826947455311959e-06),\n",
       " ('intelligently', 9.826947455311959e-06),\n",
       " ('forest', 9.826947455311959e-06),\n",
       " ('basal', 9.826947455311959e-06),\n",
       " ('rics', 9.826947455311959e-06),\n",
       " ('shallow', 9.826947455311959e-06),\n",
       " ('metastases', 9.826947455311959e-06),\n",
       " ('mnns', 9.826947455311959e-06),\n",
       " ('sub2vec', 9.826947455311959e-06),\n",
       " ('gred', 9.826947455311959e-06),\n",
       " ('dominant', 9.826947455311959e-06),\n",
       " ('bares', 9.826947455311959e-06),\n",
       " ('attenuates', 9.826947455311959e-06),\n",
       " ('sedar', 9.826947455311959e-06),\n",
       " ('satiated', 9.826947455311959e-06),\n",
       " ('gentner', 9.826947455311959e-06),\n",
       " ('repre', 9.826947455311959e-06),\n",
       " ('subtraction', 9.826947455311959e-06),\n",
       " ('cooccur', 9.826947455311959e-06),\n",
       " ('wiedermann', 9.826947455311959e-06),\n",
       " ('1615', 9.826947455311959e-06),\n",
       " ('witnessed', 9.826947455311959e-06),\n",
       " ('replicating', 9.826947455311959e-06),\n",
       " ('bagged', 9.826947455311959e-06),\n",
       " ('posits', 9.826947455311959e-06),\n",
       " ('pribot', 9.826947455311959e-06),\n",
       " ('bible', 9.826947455311959e-06),\n",
       " ('boils', 9.826947455311959e-06),\n",
       " ('timepoint', 9.826947455311959e-06),\n",
       " ('preparations', 9.826947455311959e-06),\n",
       " ('deceptively', 9.826947455311959e-06),\n",
       " ('cheaper', 9.826947455311959e-06),\n",
       " ('sicily', 9.826947455311959e-06),\n",
       " ('l_q', 9.826947455311959e-06),\n",
       " ('candy', 9.826947455311959e-06),\n",
       " ('caster', 9.826947455311959e-06),\n",
       " ('9001', 9.826947455311959e-06),\n",
       " ('tween', 9.826947455311959e-06),\n",
       " ('pivo', 9.826947455311959e-06),\n",
       " ('harmoniously', 9.826947455311959e-06),\n",
       " ('koopmans', 9.826947455311959e-06),\n",
       " ('underscores', 9.826947455311959e-06),\n",
       " ('prédiction', 9.826947455311959e-06),\n",
       " ('shortly', 9.826947455311959e-06),\n",
       " ('pralong', 9.826947455311959e-06),\n",
       " ('vo', 9.826947455311959e-06),\n",
       " ('(``', 9.826947455311959e-06),\n",
       " ('fortresses', 9.826947455311959e-06),\n",
       " ('seidenfeld', 9.826947455311959e-06),\n",
       " ('errornb', 9.826947455311959e-06),\n",
       " ('nano', 9.826947455311959e-06),\n",
       " ('1874', 9.826947455311959e-06),\n",
       " ('healthier', 9.826947455311959e-06),\n",
       " ('28mm', 9.826947455311959e-06),\n",
       " ('vidosat', 9.826947455311959e-06),\n",
       " ('times3', 9.826947455311959e-06),\n",
       " ('adept', 9.826947455311959e-06),\n",
       " ('42', 9.826947455311959e-06),\n",
       " ('fabula', 9.826947455311959e-06),\n",
       " ('ostrich', 9.826947455311959e-06),\n",
       " ('s92', 9.826947455311959e-06),\n",
       " ('scargle', 9.826947455311959e-06),\n",
       " ('neuraghe', 9.826947455311959e-06),\n",
       " ('miniaturization', 9.826947455311959e-06),\n",
       " ('wn11', 9.826947455311959e-06),\n",
       " ('analytic', 9.826947455311959e-06),\n",
       " ('hartry', 9.826947455311959e-06),\n",
       " ('catapults', 9.826947455311959e-06),\n",
       " ('akaike', 9.826947455311959e-06),\n",
       " ('wnp', 9.826947455311959e-06),\n",
       " ('structurized', 9.826947455311959e-06),\n",
       " ('putative', 9.826947455311959e-06),\n",
       " ('coinbonmin', 9.826947455311959e-06),\n",
       " ('dsrim', 9.826947455311959e-06),\n",
       " ('imputed', 9.826947455311959e-06),\n",
       " ('msh', 9.826947455311959e-06),\n",
       " ('booklet', 9.826947455311959e-06),\n",
       " ('icebergs', 9.826947455311959e-06),\n",
       " ('somehow', 9.826947455311959e-06),\n",
       " ('timeseries', 9.826947455311959e-06),\n",
       " ('baum', 9.826947455311959e-06),\n",
       " ('concluded', 9.826947455311959e-06),\n",
       " ('biol', 9.826947455311959e-06),\n",
       " ('fpba', 9.826947455311959e-06),\n",
       " ('unlabelled', 9.826947455311959e-06),\n",
       " ('*}$.', 9.826947455311959e-06),\n",
       " ('349', 9.826947455311959e-06),\n",
       " ('octree', 9.826947455311959e-06),\n",
       " ('interactively', 9.826947455311959e-06),\n",
       " ('1408', 9.826947455311959e-06),\n",
       " ('hormones', 9.826947455311959e-06),\n",
       " ('cvlab', 9.826947455311959e-06),\n",
       " ('unloading', 9.826947455311959e-06),\n",
       " ('advent', 9.826947455311959e-06),\n",
       " ('ddqn', 9.826947455311959e-06),\n",
       " ('mer', 9.826947455311959e-06),\n",
       " ('wirelessly', 9.826947455311959e-06),\n",
       " ('1608', 9.826947455311959e-06),\n",
       " ('scnns', 9.826947455311959e-06),\n",
       " ('ccrbm', 9.826947455311959e-06),\n",
       " ('christian', 9.826947455311959e-06),\n",
       " ('yeti', 9.826947455311959e-06),\n",
       " ('redundancies', 9.826947455311959e-06),\n",
       " ('throughs', 9.826947455311959e-06),\n",
       " ('aircrafts', 9.826947455311959e-06),\n",
       " (')})$.', 9.826947455311959e-06),\n",
       " ('bitrates', 9.826947455311959e-06),\n",
       " ('stray', 9.826947455311959e-06),\n",
       " ('printing', 9.826947455311959e-06),\n",
       " ('apga', 9.826947455311959e-06),\n",
       " ('fung', 9.826947455311959e-06),\n",
       " ('valverde', 9.826947455311959e-06),\n",
       " ('arrive', 9.826947455311959e-06),\n",
       " ('autobagging', 9.826947455311959e-06),\n",
       " ('fixate', 9.826947455311959e-06),\n",
       " ('exponents', 9.826947455311959e-06),\n",
       " ('bold', 9.826947455311959e-06),\n",
       " ('graphene', 9.826947455311959e-06),\n",
       " ('query', 9.826947455311959e-06),\n",
       " ('naka', 9.826947455311959e-06),\n",
       " ('ood', 9.826947455311959e-06),\n",
       " ('pelu', 9.826947455311959e-06),\n",
       " ('bendable', 9.826947455311959e-06),\n",
       " ('recovers', 9.826947455311959e-06),\n",
       " ('nrevss', 9.826947455311959e-06),\n",
       " ('colorblind', 9.826947455311959e-06),\n",
       " ('insofar', 9.826947455311959e-06),\n",
       " ('avx2', 9.826947455311959e-06),\n",
       " ('phonotactical', 9.826947455311959e-06),\n",
       " ('04777', 9.826947455311959e-06),\n",
       " ('quinphones', 9.826947455311959e-06),\n",
       " ('simulate', 9.826947455311959e-06),\n",
       " ('nodulex', 9.826947455311959e-06),\n",
       " ('pcms', 9.826947455311959e-06),\n",
       " ('sade', 9.826947455311959e-06),\n",
       " ('whatknowledge', 9.826947455311959e-06),\n",
       " ('bm', 9.826947455311959e-06),\n",
       " ('stumps', 9.826947455311959e-06),\n",
       " ('eurocentric', 9.826947455311959e-06),\n",
       " ('variatonal', 9.826947455311959e-06),\n",
       " ('answer', 9.826947455311959e-06),\n",
       " ('10691', 9.826947455311959e-06),\n",
       " ('bayer', 9.826947455311959e-06),\n",
       " ('overthe', 9.826947455311959e-06),\n",
       " ('visualiser', 9.826947455311959e-06),\n",
       " ('hodgkin', 9.826947455311959e-06),\n",
       " ('israel', 9.826947455311959e-06),\n",
       " ('nll', 9.826947455311959e-06),\n",
       " (')}),', 9.826947455311959e-06),\n",
       " ('marginover', 9.826947455311959e-06),\n",
       " ('swih', 9.826947455311959e-06),\n",
       " ('transcriptome', 9.826947455311959e-06),\n",
       " ('toponym', 9.826947455311959e-06),\n",
       " ('implicational', 9.826947455311959e-06),\n",
       " ('stereohomology', 9.826947455311959e-06),\n",
       " ('exemplified', 9.826947455311959e-06),\n",
       " ('boiled', 9.826947455311959e-06),\n",
       " ('dialectometric', 9.826947455311959e-06),\n",
       " ('ptb', 9.826947455311959e-06),\n",
       " ('cutset', 9.826947455311959e-06),\n",
       " ('coordintate', 9.826947455311959e-06),\n",
       " ('munroe', 9.826947455311959e-06),\n",
       " ('faiss', 9.826947455311959e-06),\n",
       " ('microsystem', 9.826947455311959e-06),\n",
       " ('nine', 9.826947455311959e-06),\n",
       " ('carleman', 9.826947455311959e-06),\n",
       " ('catalogues', 9.826947455311959e-06),\n",
       " ('predictivity', 9.826947455311959e-06),\n",
       " ('reconstructing', 9.826947455311959e-06),\n",
       " ('facet', 9.826947455311959e-06),\n",
       " ('plants', 9.826947455311959e-06),\n",
       " ('rail', 9.826947455311959e-06),\n",
       " ('mmre', 9.826947455311959e-06),\n",
       " ('valeur', 9.826947455311959e-06),\n",
       " ('pdlsl', 9.826947455311959e-06),\n",
       " ('produces', 9.826947455311959e-06),\n",
       " ('neuroprosthetic', 9.826947455311959e-06),\n",
       " ('censor', 9.826947455311959e-06),\n",
       " ('disbursement', 9.826947455311959e-06),\n",
       " ('vert_1', 9.826947455311959e-06),\n",
       " ('prelu', 9.826947455311959e-06),\n",
       " ('restored', 9.826947455311959e-06),\n",
       " ('zanjan', 9.826947455311959e-06),\n",
       " ('arise', 9.826947455311959e-06),\n",
       " ('chauvin', 9.826947455311959e-06),\n",
       " ('automorphic', 9.826947455311959e-06),\n",
       " ('extolled', 9.826947455311959e-06),\n",
       " ('524', 9.826947455311959e-06),\n",
       " ('gat', 9.826947455311959e-06),\n",
       " ('10db', 9.826947455311959e-06),\n",
       " ('immediacy', 9.826947455311959e-06),\n",
       " ('arpeggio', 9.826947455311959e-06),\n",
       " ('convsrc', 9.826947455311959e-06),\n",
       " ('stays', 9.826947455311959e-06),\n",
       " ('intermediary', 9.826947455311959e-06),\n",
       " ('75x', 9.826947455311959e-06),\n",
       " ('examinees', 9.826947455311959e-06),\n",
       " ('detailed', 9.826947455311959e-06),\n",
       " ('bouligand', 9.826947455311959e-06),\n",
       " ('traveller', 9.826947455311959e-06),\n",
       " ('scenecut', 9.826947455311959e-06),\n",
       " ('lrimlc15', 9.826947455311959e-06),\n",
       " ('bias', 9.826947455311959e-06),\n",
       " ('anxiety', 9.826947455311959e-06),\n",
       " ('geos', 9.826947455311959e-06),\n",
       " ('guis', 9.826947455311959e-06),\n",
       " ('ized', 9.826947455311959e-06),\n",
       " ('mggp', 9.826947455311959e-06),\n",
       " ('huxley', 9.826947455311959e-06),\n",
       " ('checkpointed', 9.826947455311959e-06),\n",
       " ('empiricists', 9.826947455311959e-06),\n",
       " ('instrumentaltests', 9.826947455311959e-06),\n",
       " ('insufficiently', 9.826947455311959e-06),\n",
       " ('particulate', 9.826947455311959e-06),\n",
       " ('screening', 9.826947455311959e-06),\n",
       " ('gcwa', 9.826947455311959e-06),\n",
       " ('fst', 9.826947455311959e-06),\n",
       " ('watched', 9.826947455311959e-06),\n",
       " ('lithium', 9.826947455311959e-06),\n",
       " ('adopted', 9.826947455311959e-06),\n",
       " ('749', 9.826947455311959e-06),\n",
       " ('contributing', 9.826947455311959e-06),\n",
       " ('dhf1k', 9.826947455311959e-06),\n",
       " ('spcs', 9.826947455311959e-06),\n",
       " ('co2', 9.826947455311959e-06),\n",
       " ('unrevealed', 9.826947455311959e-06),\n",
       " ('melodies', 9.826947455311959e-06),\n",
       " ('might', 9.826947455311959e-06),\n",
       " ('ameliorate', 9.826947455311959e-06),\n",
       " ('dissortative', 9.826947455311959e-06),\n",
       " ('musa2', 9.826947455311959e-06),\n",
       " ('aesw', 9.826947455311959e-06),\n",
       " ('spacenet', 9.826947455311959e-06),\n",
       " ('matbn', 9.826947455311959e-06),\n",
       " ('chartered', 9.826947455311959e-06),\n",
       " ('90x', 9.826947455311959e-06),\n",
       " ('whorl', 9.826947455311959e-06),\n",
       " ('unemotional', 9.826947455311959e-06),\n",
       " ('fastprim', 9.826947455311959e-06),\n",
       " ('dmse', 9.826947455311959e-06),\n",
       " ('imagenet64', 9.826947455311959e-06),\n",
       " ('axiomatizationof', 9.826947455311959e-06),\n",
       " ('outsource', 9.826947455311959e-06),\n",
       " ('adelman', 9.826947455311959e-06),\n",
       " ('gscad', 9.826947455311959e-06),\n",
       " ('obfuscation', 9.826947455311959e-06),\n",
       " ('20x', 9.826947455311959e-06),\n",
       " ('fingercode', 9.826947455311959e-06),\n",
       " ('guajarati', 9.826947455311959e-06),\n",
       " ('appeals', 9.826947455311959e-06),\n",
       " ('cise', 9.826947455311959e-06),\n",
       " ('callfriend', 9.826947455311959e-06),\n",
       " ('palro', 9.826947455311959e-06),\n",
       " ('dampened', 9.826947455311959e-06),\n",
       " ('convoluting', 9.826947455311959e-06),\n",
       " ('verbs', 9.826947455311959e-06),\n",
       " ('euphony', 9.826947455311959e-06),\n",
       " ('anno', 9.826947455311959e-06),\n",
       " ('lubachevsky', 9.826947455311959e-06),\n",
       " ('raising', 9.826947455311959e-06),\n",
       " ('feta', 9.826947455311959e-06),\n",
       " ('redefining', 9.826947455311959e-06),\n",
       " ('iahcc', 9.826947455311959e-06),\n",
       " ('chanics', 9.826947455311959e-06),\n",
       " ('mathml', 9.826947455311959e-06),\n",
       " (':[', 9.826947455311959e-06),\n",
       " ('retargeting', 9.826947455311959e-06),\n",
       " ('cement', 9.826947455311959e-06),\n",
       " ('radially', 9.826947455311959e-06),\n",
       " ('gsrc', 9.826947455311959e-06),\n",
       " ('iganaki', 9.826947455311959e-06),\n",
       " ('corruption', 9.826947455311959e-06),\n",
       " ('anns', 9.826947455311959e-06),\n",
       " ('teknikal', 9.826947455311959e-06),\n",
       " ('anticas', 9.826947455311959e-06),\n",
       " ('lyapunov', 9.826947455311959e-06),\n",
       " ('phi', 9.826947455311959e-06),\n",
       " ('timedomain', 9.826947455311959e-06),\n",
       " ('diversified', 9.826947455311959e-06),\n",
       " ('cpu', 9.826947455311959e-06),\n",
       " ('reaffirms', 9.826947455311959e-06),\n",
       " ('espnet', 9.826947455311959e-06),\n",
       " ('multilayerperception', 9.826947455311959e-06),\n",
       " ('indefinable', 9.826947455311959e-06),\n",
       " ('biphones', 9.826947455311959e-06),\n",
       " ('prohibitively', 9.826947455311959e-06),\n",
       " ('rolled', 9.826947455311959e-06),\n",
       " ('ignored', 9.826947455311959e-06),\n",
       " ('layer', 9.826947455311959e-06),\n",
       " ('epg', 9.826947455311959e-06),\n",
       " ('gilbert', 9.826947455311959e-06),\n",
       " ('disseminate', 9.826947455311959e-06),\n",
       " ('1705', 9.826947455311959e-06),\n",
       " ('distributedby', 9.826947455311959e-06),\n",
       " ('scop', 9.826947455311959e-06),\n",
       " ('coxph', 9.826947455311959e-06),\n",
       " ('rechargeable', 9.826947455311959e-06),\n",
       " ('meaningful', 9.826947455311959e-06),\n",
       " ('couzin', 9.826947455311959e-06),\n",
       " ('ciag', 9.826947455311959e-06),\n",
       " ('frpc', 9.826947455311959e-06),\n",
       " ('seldin', 9.826947455311959e-06),\n",
       " ('mechanistic', 9.826947455311959e-06),\n",
       " ('deepem', 9.826947455311959e-06),\n",
       " ('comparative', 9.826947455311959e-06),\n",
       " ('compass', 9.826947455311959e-06),\n",
       " ('byte', 9.826947455311959e-06),\n",
       " ('interception', 9.826947455311959e-06),\n",
       " ('objectnet3d', 9.826947455311959e-06),\n",
       " ('posetrack', 9.826947455311959e-06),\n",
       " ('dkr', 9.826947455311959e-06),\n",
       " ('reblur2deblur', 9.826947455311959e-06),\n",
       " ('coderx7', 9.826947455311959e-06),\n",
       " ('multidomain', 9.826947455311959e-06),\n",
       " ('mobility', 9.826947455311959e-06),\n",
       " ('offsetnet', 9.826947455311959e-06),\n",
       " ('bn', 9.826947455311959e-06),\n",
       " ('sequencer', 9.826947455311959e-06),\n",
       " ('unsociable', 9.826947455311959e-06),\n",
       " ('disproves', 9.826947455311959e-06),\n",
       " ('converting', 9.826947455311959e-06),\n",
       " ('tendering', 9.826947455311959e-06),\n",
       " ('hitters', 9.826947455311959e-06),\n",
       " ('namesake', 9.826947455311959e-06),\n",
       " ('hurdles', 9.826947455311959e-06),\n",
       " ('euanguages', 9.826947455311959e-06),\n",
       " ('teodor', 9.826947455311959e-06),\n",
       " ('61158', 9.826947455311959e-06),\n",
       " ('iecbs', 9.826947455311959e-06),\n",
       " ('photosensors', 9.826947455311959e-06),\n",
       " ('paradigm', 9.826947455311959e-06),\n",
       " ('morph', 9.826947455311959e-06),\n",
       " ('openxbow', 9.826947455311959e-06),\n",
       " ('exploitatory', 9.826947455311959e-06),\n",
       " ('62k', 9.826947455311959e-06),\n",
       " ('crystallography', 9.826947455311959e-06),\n",
       " ('administrator', 9.826947455311959e-06),\n",
       " ('csec', 9.826947455311959e-06),\n",
       " ('vice', 9.826947455311959e-06),\n",
       " ('rc3d', 9.826947455311959e-06),\n",
       " ('cyberattack', 9.826947455311959e-06),\n",
       " ('mfqe', 9.826947455311959e-06),\n",
       " ('converted', 9.826947455311959e-06),\n",
       " ('rosa', 9.826947455311959e-06),\n",
       " ('ioflupane', 9.826947455311959e-06),\n",
       " ('bugassist', 9.826947455311959e-06),\n",
       " ('lineup', 9.826947455311959e-06),\n",
       " ('grf', 9.826947455311959e-06),\n",
       " ('tfrac', 9.826947455311959e-06),\n",
       " ('scores', 9.826947455311959e-06),\n",
       " ('modular', 9.826947455311959e-06),\n",
       " ('dezert', 9.826947455311959e-06),\n",
       " ('nlpr', 9.826947455311959e-06),\n",
       " ('mislocalizations', 9.826947455311959e-06),\n",
       " ('ignoring', 9.826947455311959e-06),\n",
       " ('pmcs', 9.826947455311959e-06),\n",
       " ('unwelcome', 9.826947455311959e-06),\n",
       " ('isotropic', 9.826947455311959e-06),\n",
       " ('andberend', 9.826947455311959e-06),\n",
       " ('togetherness', 9.826947455311959e-06),\n",
       " ('benders', 9.826947455311959e-06),\n",
       " ('diverge', 9.826947455311959e-06),\n",
       " ('display', 9.826947455311959e-06),\n",
       " ('slo', 9.826947455311959e-06),\n",
       " ('primary', 9.826947455311959e-06),\n",
       " ('}^{-\\\\', 9.826947455311959e-06),\n",
       " ('multileveled', 9.826947455311959e-06),\n",
       " ('dumps', 9.826947455311959e-06),\n",
       " ('aops', 9.826947455311959e-06),\n",
       " ('averts', 9.826947455311959e-06),\n",
       " ('p4m', 9.826947455311959e-06),\n",
       " ('supervisors', 9.826947455311959e-06),\n",
       " ('thinker', 9.826947455311959e-06),\n",
       " ('maxdnn', 9.826947455311959e-06),\n",
       " ('biodiversity', 9.826947455311959e-06),\n",
       " ('1981', 9.826947455311959e-06),\n",
       " ('compatibility', 9.826947455311959e-06),\n",
       " ('tokenizing', 9.826947455311959e-06),\n",
       " ('alliterative', 9.826947455311959e-06),\n",
       " ('ic50', 9.826947455311959e-06),\n",
       " ('dxnat', 9.826947455311959e-06),\n",
       " ('chater', 9.826947455311959e-06),\n",
       " ('intercellular', 9.826947455311959e-06),\n",
       " ('text2shape', 9.826947455311959e-06),\n",
       " ('flr', 9.826947455311959e-06),\n",
       " ('arriving', 9.826947455311959e-06),\n",
       " ('energetic', 9.826947455311959e-06),\n",
       " ('nonlinearity', 9.826947455311959e-06),\n",
       " ('fstp', 9.826947455311959e-06),\n",
       " ('emulated', 9.826947455311959e-06),\n",
       " ('fullreference', 9.826947455311959e-06),\n",
       " ('kivinen', 9.826947455311959e-06),\n",
       " ('>>,', 9.826947455311959e-06),\n",
       " ('slavic', 9.826947455311959e-06),\n",
       " ('unsu', 9.826947455311959e-06),\n",
       " ('cholecystectomy', 9.826947455311959e-06),\n",
       " ('micrornas', 9.826947455311959e-06),\n",
       " ('procedureto', 9.826947455311959e-06),\n",
       " ('cosmic', 9.826947455311959e-06),\n",
       " ('ample', 9.826947455311959e-06),\n",
       " ('kp', 9.826947455311959e-06),\n",
       " ('suppose', 9.826947455311959e-06),\n",
       " ('edgelets', 9.826947455311959e-06),\n",
       " ('conserved', 9.826947455311959e-06),\n",
       " ('hazing', 9.826947455311959e-06),\n",
       " ('recs', 9.826947455311959e-06),\n",
       " ('coopnets', 9.826947455311959e-06),\n",
       " ('csdg', 9.826947455311959e-06),\n",
       " ('presuming', 9.826947455311959e-06),\n",
       " ('migrated', 9.826947455311959e-06),\n",
       " ('remplissage', 9.826947455311959e-06),\n",
       " ('shar', 9.826947455311959e-06),\n",
       " ('confusers', 9.826947455311959e-06),\n",
       " ('initial', 9.826947455311959e-06),\n",
       " ('unimprovable', 9.826947455311959e-06),\n",
       " ('pvanet', 9.826947455311959e-06),\n",
       " ('swansea', 9.826947455311959e-06),\n",
       " (')})', 9.826947455311959e-06),\n",
       " ('authorize', 9.826947455311959e-06),\n",
       " ('cardioverter', 9.826947455311959e-06),\n",
       " ('dda', 9.826947455311959e-06),\n",
       " ('equalize', 9.826947455311959e-06),\n",
       " ('seebyte', 9.826947455311959e-06),\n",
       " ('};', 9.826947455311959e-06),\n",
       " ('minka', 9.826947455311959e-06),\n",
       " ('fertilize', 9.826947455311959e-06),\n",
       " ('certifier', 9.826947455311959e-06),\n",
       " ('1115', 9.826947455311959e-06),\n",
       " ('malmo', 9.826947455311959e-06),\n",
       " ('ockham', 9.826947455311959e-06),\n",
       " ('ppfolio', 9.826947455311959e-06),\n",
       " ('fwa', 9.826947455311959e-06),\n",
       " ('ficient', 9.826947455311959e-06),\n",
       " ('_dr2', 9.826947455311959e-06),\n",
       " ('neuromatrices', 9.826947455311959e-06),\n",
       " ('addressing', 9.826947455311959e-06),\n",
       " ('mlk', 9.826947455311959e-06),\n",
       " ('drlmaca', 9.826947455311959e-06),\n",
       " ('undocumented', 9.826947455311959e-06),\n",
       " ('origindestination', 9.826947455311959e-06),\n",
       " ('pureed', 9.826947455311959e-06),\n",
       " ('operand', 9.826947455311959e-06),\n",
       " ('cess', 9.826947455311959e-06),\n",
       " ('mx', 9.826947455311959e-06),\n",
       " ('ilds', 9.826947455311959e-06),\n",
       " ('abducing', 9.826947455311959e-06),\n",
       " ('coreset', 9.826947455311959e-06),\n",
       " ('slant', 9.826947455311959e-06),\n",
       " ('aspocp', 9.826947455311959e-06),\n",
       " ('bayesian', 9.826947455311959e-06),\n",
       " ('magical', 9.826947455311959e-06),\n",
       " ('md4', 9.826947455311959e-06),\n",
       " ('ensures', 9.826947455311959e-06),\n",
       " ('rboosting', 9.826947455311959e-06),\n",
       " ('canteen', 9.826947455311959e-06),\n",
       " ('browsed', 9.826947455311959e-06),\n",
       " ('gbrm', 9.826947455311959e-06),\n",
       " ('foreshortening', 9.826947455311959e-06),\n",
       " ('pelov', 9.826947455311959e-06),\n",
       " ('cleanliness', 9.826947455311959e-06),\n",
       " ('intesn', 9.826947455311959e-06),\n",
       " ('cyclades', 9.826947455311959e-06),\n",
       " ('shearlets', 9.826947455311959e-06),\n",
       " ('64x64', 9.826947455311959e-06),\n",
       " ('heidelberg', 9.826947455311959e-06),\n",
       " ('intermittency', 9.826947455311959e-06),\n",
       " ('multigraphs', 9.826947455311959e-06),\n",
       " ('purl', 9.826947455311959e-06),\n",
       " ('narrativisation', 9.826947455311959e-06),\n",
       " ('clusions', 9.826947455311959e-06),\n",
       " ('ordinary', 9.826947455311959e-06),\n",
       " ('standardisation', 9.826947455311959e-06),\n",
       " ('atns', 9.826947455311959e-06),\n",
       " ('constitutions', 9.826947455311959e-06),\n",
       " ('smokon03', 9.826947455311959e-06),\n",
       " ('reconnaissance', 9.826947455311959e-06),\n",
       " ('fej', 9.826947455311959e-06),\n",
       " ('unjustifiably', 9.826947455311959e-06),\n",
       " ('overtaking', 9.826947455311959e-06),\n",
       " ('5735', 9.826947455311959e-06),\n",
       " ('instruction', 9.826947455311959e-06),\n",
       " ('effectivenes', 9.826947455311959e-06),\n",
       " ('viscosity', 9.826947455311959e-06),\n",
       " ('rhetorical', 9.826947455311959e-06),\n",
       " ('touchpads', 9.826947455311959e-06),\n",
       " ('schraudolph', 9.826947455311959e-06),\n",
       " ('spt', 9.826947455311959e-06),\n",
       " ('correctly', 9.826947455311959e-06),\n",
       " ('abilities', 9.826947455311959e-06),\n",
       " ('destined', 9.826947455311959e-06),\n",
       " ('duo', 9.826947455311959e-06),\n",
       " ('tas', 9.826947455311959e-06),\n",
       " ('exercised', 9.826947455311959e-06),\n",
       " ('land', 9.826947455311959e-06),\n",
       " ('pap', 9.826947455311959e-06),\n",
       " ('crqa', 9.826947455311959e-06),\n",
       " ('cangelosi', 9.826947455311959e-06),\n",
       " ('gong2013', 9.826947455311959e-06),\n",
       " ('32x32x3', 9.826947455311959e-06),\n",
       " ('psychophysiological', 9.826947455311959e-06),\n",
       " ...]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k, v) for k, v in lm.get_possible_next_tokens(\"bridging the\").items()], key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the source code , and the size of the proposed method produces state - of - the - art methods . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the outputs of the model to predict the next one . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: try to sample tokens from such a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser-Ney smoothing\n",
    "\n",
    "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
    "\n",
    "\n",
    "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
    "\n",
    "It can be computed recurrently, for n>1:\n",
    "\n",
    "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
    "\n",
    "where\n",
    "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
    "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
    "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
    "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
    "\n",
    "See lecture slides or wiki for more detailed formulae.\n",
    "\n",
    "__Your task__ is to\n",
    "- implement KneserNeyLanguageModel\n",
    "- test it on 1-3 gram language models\n",
    "- find optimal (within one order of magnitude) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
    "    def __init__(self, lines, n, delta=5.0):\n",
    "        self.n = n\n",
    "        \n",
    "        self.counts = {i: count_ngrams(lines, i) for i in range(1, self.n + 1)}\n",
    "        self.vocab = set(token for token_counts in self.counts[1].values() for token in token_counts)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(defaultdict(Counter))\n",
    "#         self.probs = self.counts.copy()\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for prefix in self.counts.keys():\n",
    "            for i in range(1, self.n + 1):\n",
    "                token_counts = counts[i][prefix[self.n - i:]]\n",
    "                total_count = sum(token_counts.values())# + delta * len(self.vocab)\n",
    "                self.probs[i][prefix] = {\n",
    "                    token: max(0, token_counts[token] - delta) / total_count\n",
    "                    for token in token_counts\n",
    "                }\n",
    "            \n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "            \n",
    "            \n",
    "            \n",
    "            norm_coef = sum(self.probs[prefix][word] for word in self.probs[prefix].keys())\n",
    "            for word in self.probs[prefix].keys():\n",
    "                self.probs[prefix][word] /= norm_coef\n",
    "        \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        < YOUR CODE >\n",
    "        \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        self.counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "#         self.probs = defaultdict(Counter)\n",
    "        self.probs = self.counts.copy()\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for prefix in self.counts.keys():\n",
    "            norm_coef = sum(self.probs[prefix][word] for word in self.probs[prefix].keys())\n",
    "            for word in self.probs[prefix].keys():\n",
    "                self.probs[prefix][word] /= norm_coef\n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        return self.probs[self._preproc_prefix(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)\n",
    "    \n",
    "    def _preproc_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a tuple of last tokens (n - 1) tokens in prefix\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return tuple(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, smoothing=<...>)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
